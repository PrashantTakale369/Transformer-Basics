{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Creating Tokens"
      ],
      "metadata": {
        "id": "YanqRw-NGniz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The print command prints the total number of characters followed by the first 100 characters of this file for illustration purposes."
      ],
      "metadata": {
        "id": "l6YbcqRcGwPj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bmp1wDkGjJe",
        "outputId": "1d79a708-44ce-4802-9c4a-e191939a333b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 148208\n",
            "TITLE: Alice's Adventures in Wonderland\n",
            "AUTHOR: Lewis Carroll\n",
            "\n",
            "\n",
            "= CHAPTER I = \n",
            "=( Down the Rabbit-H\n"
          ]
        }
      ],
      "source": [
        "with open(\"alice_in_wonderland.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using some simple example text, we can use the re.split command with the following syntax to split a text on whitespace characters:"
      ],
      "metadata": {
        "id": "J5pgu84AYQPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Y_14FLWGpk-",
        "outputId": "10357712-01fd-4470-87fc-496171f83fe9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using some simple example text, we can use the re.split command with the following syntax to split a text on whitespace characters:"
      ],
      "metadata": {
        "id": "Jvj2uOvIYUb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gdlp5RLiX1gy",
        "outputId": "dcf0b8dd-2b85-4c35-8d68-d395f24bde97"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's modify the regular expression (re) splits on whitespaces (\\s) and commas, and periods ([,.]):\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Zu30jnSdYbVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZOujhJDYZBr",
        "outputId": "c45eacbf-7dd9-43eb-922b-060d1bd4fd3e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A small remaining issue is that the list still includes whitespace characters. Optionally, we can remove these redundant characters safely as follows:\n"
      ],
      "metadata": {
        "id": "XCcZ2uPWY0Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = [item for item in result if item.strip()]      #  item.strip() :- Removing the WhiteSpaces\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "widynVySYhuL",
        "outputId": "8c224f7f-e4d2-493f-86d9-f634e41c163f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Let's modify it a bit further so that it can also handle other types of punctuation, such as question marks, quotation marks, and the double-dashes we have seen earlier in the first 100 characters of Edith Wharton's short story, along with additional special characters:"
      ],
      "metadata": {
        "id": "Giid-i7IZFHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a-ozORQY19d",
        "outputId": "f7a76040-033d-4fb7-a2c2-de6355a8a0e2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Strip whitespace from each item and then filter out any empty strings.\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ieLCYplZGvh",
        "outputId": "72af6eb9-9e48-46ea-a5a5-8ad7f66598e2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now that we got a basic tokenizer working,"
      ],
      "metadata": {
        "id": "TwTd9A5IZmAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrqIs7cQZLBY",
        "outputId": "95e75a7b-1493-42c5-99da-d36165f894eb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['TITLE', ':', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'AUTHOR', ':', 'Lewis', 'Carroll', '=', 'CHAPTER', 'I', '=', '=', '(', 'Down', 'the', 'Rabbit-Hole', ')', '=', 'Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(preprocessed))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsrwiyW7ZuhI",
        "outputId": "6d831aa8-120a-44f6-9267-08b40c8af410"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if we want to remove the Numerical values form the Dataset then we can also do it here then after that creating the token id"
      ],
      "metadata": {
        "id": "Bg6m9utVa0Cq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Creating Token IDs"
      ],
      "metadata": {
        "id": "EUcKrjX6Z_Iq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous section, we tokenized a short story and assigned it to a Python variable called preprocessed. Let's now create a list of all unique tokens and sort them alphabetically to determine the vocabulary size:"
      ],
      "metadata": {
        "id": "Txc1HYilaKLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HtGfwVNZ8FB",
        "outputId": "6efc1476-5faa-40fa-a588-e78d0077f5f7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After determining that the vocabulary size is 3189 via the above code, we create the vocabulary and print its first 51 entries for illustration purposes:"
      ],
      "metadata": {
        "id": "yYDWCWIsaVXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}   # assinning the token id to the vocab"
      ],
      "metadata": {
        "id": "h59iOegkaQzg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZNxgaNaaey0",
        "outputId": "c639650e-5192-4acd-d7f9-e0649f20e81f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "('*', 5)\n",
            "(',', 6)\n",
            "('--', 7)\n",
            "('.', 8)\n",
            "('0', 9)\n",
            "('00', 10)\n",
            "('10', 11)\n",
            "('124', 12)\n",
            "('1865-11-26', 13)\n",
            "('2', 14)\n",
            "('2021-03-08', 15)\n",
            "('2021-08-03', 16)\n",
            "('30', 17)\n",
            "('5', 18)\n",
            "('500', 19)\n",
            "('8', 20)\n",
            "('9', 21)\n",
            "(':', 22)\n",
            "(';', 23)\n",
            "('=', 24)\n",
            "('?', 25)\n",
            "('@', 26)\n",
            "('A', 27)\n",
            "('ALICE', 28)\n",
            "('ALL', 29)\n",
            "('AND', 30)\n",
            "('ARE', 31)\n",
            "('AT', 32)\n",
            "('AUTHOR', 33)\n",
            "('Ada', 34)\n",
            "('Adventures', 35)\n",
            "('Advice', 36)\n",
            "('After', 37)\n",
            "('Ah', 38)\n",
            "('Alas', 39)\n",
            "('Alice', 40)\n",
            "('All', 41)\n",
            "('Allow', 42)\n",
            "('Always', 43)\n",
            "('Ambition', 44)\n",
            "('An', 45)\n",
            "('And', 46)\n",
            "('Ann', 47)\n",
            "('Antipathies', 48)\n",
            "('Arithmetic', 49)\n",
            "('As', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Later in this book, when we want to convert the outputs of an LLM from numbers back into text, we also need a way to turn token IDs into text.\n",
        "\n",
        "For this, we can create an inverse version of the vocabulary that maps token IDs back to corresponding text tokens."
      ],
      "metadata": {
        "id": "WLF9L8T_bOge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The class will have an encode method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary.\n",
        "### In addition, we implement a decode method that carries out the reverse integer-to-string mapping to convert the token IDs back into text."
      ],
      "metadata": {
        "id": "_kBPfZlxbToT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Store the vocabulary as a class attribute for access in the encode and decode methods\n",
        "\n",
        "# Step 2: Create an inverse vocabulary that maps token IDs back to the original text tokens\n",
        "\n",
        "# Step 3: Process input text into token IDs\n",
        "\n",
        "# Step 4: Convert token IDs back into text\n",
        "\n",
        "# Step 5: Replace spaces before the specified punctuation"
      ],
      "metadata": {
        "id": "aCBv7q8LapgQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "zKoTSdMybzDN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's instantiate a new tokenizer object from the SimpleTokenizerV1 try it out in practice:"
      ],
      "metadata": {
        "id": "9qyjY9eTcMef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = \"\"\"\"Alice's Adventures in Wonderland\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXgTZ7cRb2yh",
        "outputId": "3c73a140-675b-4247-9d94-d2d21ed3c00d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 40, 2, 2489, 35, 1772, 447]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above prints the following token IDs: Next, let's see if we can turn these token IDs back into text using the decode method:"
      ],
      "metadata": {
        "id": "qKIWspUTeULH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "02XftDj2cbhK",
        "outputId": "ccef4b85-21a6-4f3c-9d25-53e895439788"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\" Alice\\' s Adventures in Wonderland'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, do you like tea?\"\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "MSVSK7CPeWKq",
        "outputId": "1f544e8c-8874-4d66-e8b1-3c4c23f2ed43"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Hello'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-24-1763555282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Hello, do you like tea?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-20-2118097954.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         ]\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-20-2118097954.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         ]\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Hello'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ADDING SPECIAL CONTEXT TOKENS"
      ],
      "metadata": {
        "id": "uUfvYQ4WehSy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In the previous section, we implemented a simple tokenizer and applied it to a passage from the training set.\n",
        "\n",
        "# In this section, we will modify this tokenizer to handle unknown words.\n",
        "\n",
        "# In particular, we will modify the vocabulary and tokenizer we implemented in the previous section, SimpleTokenizerV2\n",
        "# to support two new tokens, <|unk|> and <|endoftext|>"
      ],
      "metadata": {
        "id": "m04c5EQoeZM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now modify the vocabulary to include these two special tokens, and <|endoftext|>, by adding these to the list of all unique words that we created in the previous section:"
      ],
      "metadata": {
        "id": "WVqpGG9jes0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}"
      ],
      "metadata": {
        "id": "zP9wBu_JetDT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(vocab.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJMDtreHezNx",
        "outputId": "7a9ed162-9fb6-47c0-8bc9-757b8b64f50b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3191"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjYIiNePe0lp",
        "outputId": "90e60159-6da4-4374-9de8-8cf797f12f54"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('yourself', 3186)\n",
            "('youth', 3187)\n",
            "('zigzag', 3188)\n",
            "('<|endoftext|>', 3189)\n",
            "('<|unk|>', 3190)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9fRRNrkLe3kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Replace unknown words by <|unk|> tokens\n",
        "\n",
        "Step 2: Replace spaces before the specified punctuations"
      ],
      "metadata": {
        "id": "Qdm5PccBe9IN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV2:\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "        preprocessed = [\n",
        "            item if item in self.str_to_int           # <===#\n",
        "            else \"<|unk|>\" for item in preprocessed   # <===#\n",
        "        ]\n",
        "\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.:;?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "dCz9XaIQe9Ya"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a2sBS0ZfHJh",
        "outputId": "6811174e-933f-43e1-b1a0-37860572b6c4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x85EpcArfKBc",
        "outputId": "d8ed8dda-2d6d-4498-c250-d0c779baa8fc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3190,\n",
              " 6,\n",
              " 1289,\n",
              " 3182,\n",
              " 1920,\n",
              " 2820,\n",
              " 25,\n",
              " 3189,\n",
              " 198,\n",
              " 2848,\n",
              " 3190,\n",
              " 3190,\n",
              " 2139,\n",
              " 2848,\n",
              " 3190,\n",
              " 8]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Q-xNKMFkfL_9",
        "outputId": "e695f201-87a9-405c-f5d2-231c83a8fdda"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|unk|>, do you like tea? <|endoftext|> In the <|unk|> <|unk|> of the <|unk|>.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r5YU-UBdfOSf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}